{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dda7de37-ec16-4d7a-b7ff-90fde6e31442",
   "metadata": {},
   "source": [
    "# 1. 강화학습 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7dbea-df78-4be4-a794-1520690bccec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e09aed-36ce-461c-bc7e-6858eafb4f71",
   "metadata": {},
   "source": [
    "## 1.1. Agent 정책 (Policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6fc304-de89-439e-b557-798d983a85e6",
   "metadata": {},
   "source": [
    "### On-policy 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bdff10-59a8-4127-a0c6-a31a62df56e9",
   "metadata": {},
   "source": [
    "&emsp;$Q(s,a)$ 함수가 현재 정책 $π(a|s)$를 사용하여 취한 행동(action)으로부터 학습됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a56ade-cbae-4fd6-b03a-22f8acf8b42b",
   "metadata": {},
   "source": [
    "### Off-policy 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc07a887-5e5d-405a-98f3-6f121db6259e",
   "metadata": {},
   "source": [
    "&emsp;$Q(s,a)$ 함수가 다양한 행동들(예: 무작위 행동)로부터 학습됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20cec81-595f-4193-81bf-f25d3d1f8f31",
   "metadata": {},
   "source": [
    "## 1.2. 몬테 카를로 (Monte Carlo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06029144-d981-4218-9d37-164e46ce2cd9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
